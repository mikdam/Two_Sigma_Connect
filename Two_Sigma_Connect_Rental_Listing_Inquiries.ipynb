{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier\n",
    "import string\n",
    "import warnings\n",
    "from collections import Counter\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trans = string.maketrans(string.punctuation, len(string.punctuation) * ' ')\n",
    "\n",
    "RD = {}\n",
    "RD['E'] = 'EAST'\n",
    "RD['W'] = 'WEST'\n",
    "RD['N'] = 'NORTH'\n",
    "RD['S'] = 'SOUTH'\n",
    "RD['ST'] = 'STREET'\n",
    "RD['AVE'] = 'AVENUE'\n",
    "RD['AV'] = 'AVENUE'\n",
    "RD['PL'] = 'PLACE'\n",
    "RD['PKWY'] = 'PARKWAY'\n",
    "RD['RD'] = 'ROAD'\n",
    "RD['BLVD'] = 'BOULEVARD'\n",
    "RD['DR'] = 'DRIVE'\n",
    "RD['CT'] = 'COURT'\n",
    "RD['LN'] = 'LANE'\n",
    "RD['LA'] = 'LANE'\n",
    "RD['1ST'] = '1'\n",
    "RD['2ND'] = '2'\n",
    "RD['3RD'] = '3'\n",
    "RD['FIRST'] = '1'\n",
    "RD['SECOND'] = '2'\n",
    "RD['THIRD'] = '3'\n",
    "RD['FOURTH'] = '4'\n",
    "RD['FIFTH'] = '5'\n",
    "RD['SIXTH'] = '6'\n",
    "RD['SEVENTH'] = '7'\n",
    "RD['EIGHTH'] = '8'\n",
    "RD['NINTH'] = '9'\n",
    "RD['TENTH'] = '10'\n",
    "RD['ELEVENTH'] = '11'\n",
    "RD['11TH'] = '11'\n",
    "RD['12TH'] = '12'\n",
    "RD['13TH'] = '13'\n",
    "\n",
    "\n",
    "RD2 ={}\n",
    "RD2['1ST'] = '1'\n",
    "RD2['2ND'] = '2'\n",
    "RD2['3RD'] = '3'\n",
    "RD2['4TH'] = '4'\n",
    "RD2['5TH'] = '5'\n",
    "RD2['6TH'] = '6'\n",
    "RD2['7TH'] = '7'\n",
    "RD2['8TH'] = '8'\n",
    "RD2['9TH'] = '9'\n",
    "RD2['0TH'] = '0'\n",
    "\n",
    "RD3 = ['EAST', 'WEST', 'NORTH', 'SOUTH']\n",
    "sn = ['STREET', 'AVENUE', 'PLACE', 'PARKWAY', 'ROAD', 'BOULEVARD', 'DRIVE', 'COURT', 'LANE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extracting street name, eliminating any leading numbers \n",
    "def street_name(x):\n",
    "    words = x.split()\n",
    "    for s in sn:\n",
    "        if s in words:\n",
    "            if words[0].isdigit() and words.index(s)>1:\n",
    "                return ' '.join(words[1:])\n",
    "            \n",
    "    for s in RD3:\n",
    "        if s in words:\n",
    "            if words[0].isdigit() and words.index(s)==1:\n",
    "                return ' '.join(words[1:])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Coding address using a coding list for cleaned addresses\n",
    "def code_addr(addr, addr_coding_list):\n",
    "    addr = str(addr.encode('utf-8'))\n",
    "    addr = addr.translate(trans)\n",
    "    addr = addr.replace('\\n', ' ')\n",
    "    addr = addr.replace('\\r', ' ')\n",
    "    addr = addr.upper()\n",
    "    addr = addr.strip()\n",
    "    \n",
    "    addr = ' '.join([RD[w] if w in RD else w for w in addr.split()])\n",
    "    \n",
    "    for w1, w2 in RD2.iteritems():\n",
    "        if w1 in addr:\n",
    "            addr = addr.replace(w1, w2) \n",
    "    try:        \n",
    "        r =  [(code,subaddr) for (code, subaddr) in addr_coding_list if subaddr in addr]\n",
    "    except:\n",
    "        print addr, 'errr'\n",
    "        \n",
    "    if len(r) > 1:\n",
    "        r.sort(key=lambda x: len(str(x[1])), reverse=True)\n",
    "\n",
    "    return r[0][0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning addresses and creating adderss coding list\n",
    "def create_address_list(data):\n",
    "    street = list(data)\n",
    "                  \n",
    "    street = [str(x.encode('utf-8')) for x in street]\n",
    "    street = [x.translate(trans) for x in street]\n",
    "    street = [x.replace('\\n', ' ') for x in street]\n",
    "    street = [x.replace('\\r', ' ') for x in street]\n",
    "    street = [x.upper() for x in street]\n",
    "    street = [x.strip() for x in street]\n",
    "    \n",
    "    street = [' '.join([RD[w] if w in RD else w for w in x.split()]) for x in street]\n",
    "\n",
    "    for w1, w2 in RD2.iteritems():\n",
    "        street = [x.replace(w1, w2) if w1 in x else x for x in street]\n",
    "    \n",
    "    street = [street_name(x) for x in street]\n",
    "\n",
    "    street = list(set(street))\n",
    "    \n",
    "    contains =[]\n",
    "                \n",
    "    street = [x for x in street if x not in contains and not x.isdigit() and len(x) < 50 and len(x) > 3]\n",
    "    \n",
    "    l =  zip(range(len(street)), street)\n",
    "    \n",
    "    l.append((-1, ''))\n",
    "    \n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The initial features\n",
    "all_features = ['bathrooms', 'bedrooms', 'desc_length', 'features_length', 'photos_no', 'price', \n",
    "                'price_per_bedroom', 'bedroom_per_bathroom', 'day', 'hour', 'price_diff', \n",
    "                'bedrooms_diff', 'latitude', 'longitude', 'price_per_bedroom_diff', 'disp_addr', 'area']\n",
    "\n",
    "# Features that will be coded as probability of the target variable\n",
    "prob_trans_features = ['disp_addr', 'building_id', 'manager_id', 'area',\n",
    "                       'manager_id_area', 'area_price']\n",
    "\n",
    "# Features to label_encode\n",
    "label_encode_features = ['building_id', 'manager_id', 'street_address']\n",
    "\n",
    "models_dict = {}\n",
    "n_desc_topics = 10\n",
    "n_features = 70\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokeniser and lemmatiser for the text of the description featuere\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        doc = doc.encode('ascii','ignore')\n",
    "        doc = doc.translate(trans)\n",
    "        return [self.wnl.lemmatize(t) for t in doc.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Coding high categerical varibales as target variable probability using empirical Bayesian  \n",
    "from math import exp\n",
    "\n",
    "def transform_categerical_feature(train, f, high_prior_prob, medium_prior_prob):\n",
    "    _lambda = lambda n: 1.0 / (1.0 + exp( -(n - 10.0) / 1.0))\n",
    "    \n",
    "    est_high = train.groupby(f, as_index=False)['is_high'].agg({'post_prob':'mean', \n",
    "                                                                'cnt': 'count'})    \n",
    "    est_high.loc[:, 'high_prob_'+f] = est_high.apply(lambda r: (_lambda(r['cnt']) * r['post_prob']) +\n",
    "                                                 ((1-_lambda(r['cnt'])) * high_prior_prob), axis=1)\n",
    "    \n",
    "    \n",
    "    est_medium = train.groupby(f, as_index=False)['is_medium'].agg({'post_prob':'mean', \n",
    "                                                                         'cnt': 'count'})\n",
    "    est_medium.loc[:, 'medium_prob_'+f] = est_medium.apply(lambda r: (_lambda(r['cnt']) * r['post_prob']) +\n",
    "                                                 ((1-_lambda(r['cnt'])) * medium_prior_prob), axis=1)\n",
    "    \n",
    "    tran_df = pd.merge(est_high[[f, 'high_prob_'+f]], est_medium[[f, 'medium_prob_'+f]], on=f)\n",
    "        \n",
    "    return tran_df\n",
    "\n",
    "\n",
    "def add_prob_columns(data, trans_df, field):\n",
    "        \n",
    "    data = data.merge(trans_df, on=field, how='left')\n",
    "    \n",
    "    data.loc[data[field]==-1, 'high_prob_'+field] = np.random.uniform(low=0.001, high=0.5\n",
    "                                                                     ,size=(len(data.loc[data[field]==-1])))\n",
    "    data.loc[data[field]==-1, 'medium_prob_'+field] = np.random.uniform(low=0.001, high=0.5\n",
    "                                                                        ,size=(len(data.loc[data[field]==-1])))\n",
    "    \n",
    "    data.loc[np.isnan(data['high_prob_'+field]), 'high_prob_'+field] = np.random.uniform(low=0.001, high=0.5\n",
    "                                                                     ,size=(len(data.loc[np.isnan(data['high_prob_'+field])])))\n",
    "    data.loc[np.isnan(data['medium_prob_'+field]), 'medium_prob_'+field] = np.random.uniform(low=0.001, high=0.5\n",
    "                                                                        ,size=(len(data.loc[np.isnan(data['medium_prob_'+field])])))\n",
    "    addresses\n",
    "    return data\n",
    "\n",
    "    \n",
    "def add_prob_training(train_df, high_prior_prob, medium_prior_prob, field):\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    tmp = pd.DataFrame()\n",
    "    \n",
    "    for itrain, itest in kfold.split(np.zeros(len(train_df)), train_df['interest_level']):\n",
    "        trans_df = transform_categerical_feature(train_df.iloc[itrain], field, high_prior_prob, medium_prior_prob)\n",
    "        tmp = tmp.append(add_prob_columns(train_df.iloc[itest], trans_df, field))\n",
    "        \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert a list of features into a probability\n",
    "punct = string.punctuation.replace('-','')+'\\n\\r\\t'\n",
    "trans_f = string.maketrans(punct, len(punct) * ' ')\n",
    "from collections import OrderedDict\n",
    "import re\n",
    "\n",
    "cleaning_list = [\n",
    "    ('BALCONY','BALCONY'),\n",
    "    ('CENTRAL AC','AIR CONDITIONING'), \n",
    "    ('CENTRAL A C','AIR CONDITIONING'), \n",
    "    ('AIR-CONDITIONING', 'AIR CONDITIONING'),\n",
    "    ('AIR CONDITIONING', 'AIR CONDITIONING'),\n",
    "    ('CONCIERGE','DOORMAN'), \n",
    "    ('DOORMAN', 'DOORMAN'),\n",
    "    ('HARDWOOD', 'HARDWOOD'),\n",
    "    ('LAUNDRY', 'LAUNDRY'),\n",
    "    ('PRE-WAR','PREWAR'), \n",
    "    ('PREWAR','PREWAR'), \n",
    "    ('PRE WAR' , 'PREWAR'),\n",
    "    ('POST-WAR','POSTWAR'), \n",
    "    ('POSTWAR', 'POSTWAR'),  \n",
    "    ('POST WAR', 'POSTWAR'),\n",
    "    ('FITNESS','FITNESS'),\n",
    "    ('GYM', 'FITNESS'),\n",
    "    ('HEALTH CLUB', 'FITNESS'),\n",
    "    ('OUTDOOR SPACE', 'OUTDOOR SPACE'), \n",
    "    ('OUTDOOR-SPACE', 'OUTDOOR SPACE'),\n",
    "    ('ROOFDECK', 'ROOF DECK'), \n",
    "    ('ROOF-DECK', 'ROOF DECK'), \n",
    "    ('ROOF DECK','ROOF DECK'), \n",
    "    ('ROOFTOP', 'ROOF DECK'),\n",
    "    ('PATIO', 'PATIO'),\n",
    "    ('POOL', 'POOL'),\n",
    "    ('MARBLE BATH','MARBLE BATH'),\n",
    "    ('RENOVATED', 'RENOVATED'),\n",
    "    ('PARKING', 'PARKING'),\n",
    "    ('WALK IN CLOSET', 'WALK IN CLOSET'),\n",
    "    ('WASHER DRYER', 'WASHER DRYER'),\n",
    "    ('WASHER IN UNIT', 'WASHER DRYER'),\n",
    "    ('WASHER AND DRYER', 'WASHER DRYER'),\n",
    "    ('GRANITE COUNTER', 'GRANITE COUNTER'),\n",
    "    ('GRANITE KITCHEN', 'GRANITE COUNTER'),\n",
    "    ('SCREENING ROOM', 'SCREENING ROOM'),\n",
    "    ('MASSIVE LIVING-ROOM', 'LARGE LIVING ROOM'),\n",
    "    ('LARGE LIVING-ROOM', 'LARGE LIVING ROOM'),\n",
    "    ('MASSIVE LIVING ROOM', 'LARGE LIVING ROOM'),\n",
    "    ('LARGE LIVING ROOM', 'LARGE LIVING ROOM'),\n",
    "    ('HUGE LIVING-ROOM', 'LARGE LIVING ROOM'),\n",
    "    ('HUGE LIVING ROOM', 'LARGE LIVING ROOM'),\n",
    "    ('LIVING DINING ROOM','LARGE LIVING ROOM'),\n",
    "    ('BIKE STORAGE', 'BIKE STORAGE'),\n",
    "    ('BICYCLE ROOM', 'BIKE STORAGE'),\n",
    "    ('BIKE ROOM', 'BIKE STORAGE'),\n",
    "    ('BIKE STROLLER', 'BIKE STORAGE'),\n",
    "    ('DUPLEX', 'DUPLEX'),\n",
    "    ('MICROWAVE', 'MICROWAVE'),\n",
    "    ('WHEELCHAIR', 'WHEELCHAIR'),\n",
    "    ('PLAYROOM', 'PLAYROOM'),\n",
    "    ('SUBWAY', 'TRANSPORT'),\n",
    "    ('TRANSPORT', 'TRANSPORT'),\n",
    "    ('BACKYARD', 'BACKYARD'),\n",
    "    ('UTILITIES INCLUDED', 'UTILITIES INCLUDED'),\n",
    "    ('HEAT INCLUDED', 'UTILITIES INCLUDED'),\n",
    "    ('GAS INCLUDED', 'UTILITIES INCLUDED'),\n",
    "    ('WATER INCLUDED', 'UTILITIES INCLUDED'),\n",
    "    ('RESIDENT LOUNGE', 'TENANT LOUNGE'),\n",
    "    ('SOCIAL LOUNGE', 'TENANT LOUNGE')\n",
    "    ]\n",
    "\n",
    "\n",
    "def clean_features_list(features):\n",
    "    return [clean_feature(re.sub(r'\\s+', ' ',str(f.encode('utf_8')).upper().translate(trans_f).strip()))\n",
    "            for f in features]\n",
    "\n",
    "def clean_feature(f):\n",
    "    for cf in cleaning_list:\n",
    "            if cf[0] in f:\n",
    "                return cf[1]\n",
    "    return f\n",
    "\n",
    "def features_probability(data):\n",
    "    \n",
    "    features = [item for subfeaturelist in list(data.features) for item in clean_features_list(subfeaturelist)] \n",
    "    \n",
    "    features_count = {f:[0.0, 0.0, 0.0] for f,_ in  Counter(features).most_common(n=500)}\n",
    "    \n",
    "    idx = {'high':0, 'medium':1, 'low':2}\n",
    "\n",
    "    for _,fl,il in data[['features', 'interest_level']].itertuples():\n",
    "        for f in clean_features_list(fl):\n",
    "            if f in features_count:\n",
    "                features_count[f][idx[il]] += 1\n",
    "            \n",
    "    h_count = len(data.loc[data.interest_level == 'high'])\n",
    "    m_count = len(data.loc[data.interest_level == 'medium'])\n",
    "    l_count = len(data.loc[data.interest_level == 'low'])\n",
    "    \n",
    "    k = 0.05\n",
    "    features_prob = []\n",
    "    for f, c in features_count.iteritems():\n",
    "        features_prob.append((f, \n",
    "                             (k + c[0]) / (2 * k + h_count), # high probability\n",
    "                             (k + c[1] + c[2]) / (2 * k + m_count + l_count), # not high probability\n",
    "                             (k + c[1]) / (2 * k + m_count), # medium probability\n",
    "                             (k + c[0] + c[2]) / (2 * k + h_count + l_count), # not medium probability\n",
    "                             ))\n",
    "        \n",
    "    h = [i[0] for i in sorted(features_prob, key=lambda x: x[1], reverse=True)][:int(n_features * 0.5)]\n",
    "    m = [i[0] for i in sorted(features_prob, key=lambda x: x[3], reverse=True)]\n",
    "    a = h \n",
    "    \n",
    "    while len(a) < n_features:\n",
    "        if m[0] not in a:\n",
    "            a.append(m[0])\n",
    "        m = m[1:]\n",
    "        \n",
    "    return features_prob, zip(range(len(a)), a)\n",
    "\n",
    "\n",
    "def calc_features_probability(features_prob, features_list, features,  h_cnt=1, m_cnt=1, all_cnt=3):\n",
    "    h_prob = h_cnt / float(all_cnt)\n",
    "    not_h_prob = (all_cnt - h_cnt) / float(all_cnt)\n",
    "    m_prob = m_cnt / float(all_cnt)\n",
    "    not_m_prob = (all_cnt - m_cnt) / float(all_cnt)\n",
    "    \n",
    "    r = OrderedDict()\n",
    "    \n",
    "    if len(features) == 0:\n",
    "        r.update({'h': np.random.uniform(high=0.8, low=0.005), \n",
    "                  'm': np.random.uniform(high=0.8, low=0.005)})\n",
    "        r.update({'feat'+str(i):0 for i in range(n_features)})\n",
    "        return pd.Series(r)\n",
    "    \n",
    "    fl = clean_features_list(features)\n",
    "    \n",
    "    f_h_prob = 0\n",
    "    f_not_h_prob = 0\n",
    "    f_m_prob = 0\n",
    "    f_not_m_prob = 0\n",
    "    \n",
    "    coded = [0]*n_features\n",
    "    f_idx = []\n",
    "    \n",
    "    for f, h, nh, m, nm in features_prob:\n",
    "        if f in fl:\n",
    "            f_h_prob += math.log(h)\n",
    "            f_not_h_prob += math.log(nh)\n",
    "            \n",
    "            f_m_prob += math.log(m)\n",
    "            f_not_m_prob += math.log(nm)\n",
    "        else:\n",
    "            f_h_prob += math.log(1.0 - h)\n",
    "            f_not_h_prob += math.log(1.0 - nh)\n",
    "            \n",
    "            f_m_prob += math.log(1.0 - m)\n",
    "            f_not_m_prob += math.log(1.0 - nm)\n",
    "        \n",
    "    for f in fl:\n",
    "        idx = [code for (code, subfeat) in features_list if subfeat == f]\n",
    "        if len(idx) > 0:\n",
    "            coded[idx[0]] = 1    \n",
    "        \n",
    "    f_h_prob = math.exp(f_h_prob)\n",
    "    f_not_h_prob = math.exp(f_not_h_prob)\n",
    "    f_m_prob = math.exp(f_m_prob)\n",
    "    f_not_m_prob = math.exp(f_not_m_prob)\n",
    "    \n",
    "    r.update({'h' :np.random.uniform(0.0, 0.0005) + (f_h_prob * h_prob) / (f_h_prob * h_prob + f_not_h_prob * not_h_prob), \n",
    "              'm' :np.random.uniform(0.0, 0.0005) + (f_m_prob * m_prob) / (f_m_prob * m_prob + f_not_m_prob * not_m_prob)})\n",
    "    \n",
    "    r.update({'feat'+str(i): coded[i] for i in range(n_features)})\n",
    "    \n",
    "    return pd.Series(r)\n",
    "\n",
    "\n",
    "def add_feature_prob_columns(data, features_prob, features_list, training=False):\n",
    "\n",
    "    if training:\n",
    "        t = len(data)\n",
    "        h = len(data.loc[data.interest_level=='high']) \n",
    "        m = len(data.loc[data.interest_level=='medium'])\n",
    "    else:\n",
    "        t = 3\n",
    "        h = m = 1\n",
    "    \n",
    "    data[['high_prob_features', 'medium_prob_features']\n",
    "         +['feat'+str(i) for i in range(n_features)]] = data.features.apply(lambda fl: \n",
    "                                                        calc_features_probability(features_prob, \n",
    "                                                                                  features_list, fl,\n",
    "                                                                                  h, m, t))\n",
    "    return data\n",
    "\n",
    "    \n",
    "def add_feature_prob_training(train_df, features_list):\n",
    "    kfold = StratifiedKFold(n_splits=5)\n",
    "    tmp = pd.DataFrame()\n",
    "    \n",
    "    for itrain, itest in kfold.split(np.zeros(len(train_df)), train_df['interest_level']):\n",
    "        f_prob, _ = features_probability(train_df.iloc[itrain])\n",
    "        tmp = tmp.append(add_feature_prob_columns(train_df.iloc[itest], f_prob, features_list, True))\n",
    "        \n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dividing the area to small squares and number them \n",
    "def add_location_features(data):\n",
    "    \n",
    "    lat_max = 40.925\n",
    "    lat_min = 40.491\n",
    "    lon_max = -73.705\n",
    "    lon_min = -74.251\n",
    "\n",
    "    data.loc[data.latitude > lat_max, 'latitude'] = lat_max\n",
    "    data.loc[data.latitude < lat_min, 'latitude'] = lat_min\n",
    "    data.loc[data.longitude > lon_max, 'longitude'] = lon_max\n",
    "    data.loc[data.longitude < lon_min, 'longitude'] = lon_min\n",
    "\n",
    "    lon = np.floor((data.longitude.values - lon_min) / (lon_max - lon_min) * 100)\n",
    "    lat = np.floor((data.latitude.values - lat_min) / (lat_max - lat_min) * 100) \n",
    "\n",
    "    data['area'] = lat * 100 + lon\n",
    "    \n",
    "    area_stats = data.groupby('area', as_index=False).agg({'price':'mean',\n",
    "                                                           'building_id':'count',\n",
    "                                                           'bedrooms':'mean',\n",
    "                                                           'price_per_bedroom':'mean'\n",
    "                                                          })\n",
    "    \n",
    "    area_stats = area_stats.rename(columns={'price':'price_avg',\n",
    "                                            'building_id':'area_density',\n",
    "                                            'bedrooms':'bedrooms_avg',\n",
    "                                            'price_per_bedroom':'price_per_bedroom_avg'\n",
    "                                             })\n",
    "        \n",
    "    data = data.merge(area_stats, on='area', how='left')\n",
    "    \n",
    "    data['price_diff'] = data['price'].values - data['price_avg'].values\n",
    "    data['bedrooms_diff'] = data['bedrooms'].values - data['bedrooms_avg'].values\n",
    "    data['price_per_bedroom_diff'] = data['price_per_bedroom'].values - data['price_per_bedroom_avg'].values\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare variables for training the model\n",
    "def prepare_data_first(train_df, test_df):\n",
    "    \n",
    "    global models_dict\n",
    "    print 'Extracting numerical features...'\n",
    "    train_df.loc[:, 'price_per_bedroom'] = train_df.apply(lambda r: r['price'] / float(r['bedrooms']) \n",
    "                                               if r['bedrooms']>0 else r['price'], axis=1)\n",
    "\n",
    "    train_df.loc[:, 'bedroom_per_bathroom'] = train_df.apply(lambda r: r['bedrooms'] / float(r['bathrooms']) \n",
    "                                               if r['bathrooms']>0 else r['bedrooms'], axis=1)\n",
    "\n",
    "    train_df.loc[:, 'price_per_all_rooms'] = train_df.apply(lambda r: r['price'] / float(r['bathrooms']+r['bedrooms']) \n",
    "                                               if r['bedrooms']+r['bathrooms']>0 else r['price'], axis=1)\n",
    "\n",
    "    train_df.loc[:,'desc_length'] = train_df.description.apply(lambda x: len(x.split()))\n",
    "    train_df.loc[:,'features_length'] = train_df.features.apply(len)\n",
    "    train_df.loc[:,'photos_no'] = train_df.photos.apply(len)\n",
    "\n",
    "    train_df.loc[:,'created'] = pd.to_datetime(train_df.loc[:, 'created'])\n",
    "    train_df.loc[:,'day'] = train_df.created.apply(lambda d: d.day)\n",
    "    train_df.loc[:,'hour'] = train_df.created.apply(lambda d: d.hour)\n",
    "    \n",
    "    print 'Building coding list for display address...'\n",
    "    addr_coding_list = create_address_list(list(train_df.display_address) + list(test_df.display_address))\n",
    "    models_dict['addr_coding_list'] = addr_coding_list\n",
    "    \n",
    "    print 'Encoding and extracting features from \"display address\"...'\n",
    "    train_df['disp_addr'] = train_df.display_address.apply(lambda r : code_addr(r, addr_coding_list))\n",
    "    \n",
    "    print 'Adding location features...'\n",
    "    train_df = add_location_features(train_df)\n",
    "    \n",
    "    print 'Pairing features...'\n",
    "    train_df['t_price'] = np.ceil(train_df.price / 100)\n",
    "    \n",
    "    train_df['manager_id_area'] = train_df.manager_id + '-' + train_df.area.astype('str') \n",
    "    train_df['manager_id_building_id'] = train_df.manager_id + '-' + train_df.building_id\n",
    "    train_df['manager_id_display_address'] = train_df.manager_id + '-' + train_df.display_address\n",
    "    train_df['manager_id_price'] = train_df.manager_id + '-' + train_df.t_price.astype('str')\n",
    "    \n",
    "    train_df['disp_addr_price'] = train_df.disp_addr.astype('str') + '-' + train_df.t_price.astype('str')\n",
    "    train_df['display_address_price'] = train_df.display_address + '-' + train_df.t_price.astype('str')\n",
    "    train_df['area_price'] = train_df.area.astype('str') + '-' + train_df.t_price.astype('str')\n",
    "    train_df['building_id_price'] = train_df.building_id + '-' + train_df.t_price.astype('str')\n",
    "    \n",
    "    train_df['manager_id_disp_addr_price'] = train_df.manager_id + '-' + train_df.disp_addr.astype('str') + '-' + train_df.t_price.astype('str')\n",
    "    train_df['manager_id_building_id_price'] = train_df.manager_id + '-' + train_df.building_id + '-' + train_df.t_price.astype('str')\n",
    "    train_df['manager_id_area_price'] = train_df.manager_id + '-' + train_df.area.astype('str') + '-' + train_df.t_price.astype('str')\n",
    "    \n",
    "    total_count = len(train_df)\n",
    "    high_prior_prob = len(train_df.loc[train_df.interest_level == 'high']) / float(total_count)\n",
    "    medium_prior_prob = len(train_df.loc[train_df.interest_level == 'medium']) / float(total_count)\n",
    "    \n",
    "    train_df.loc[:, 'is_high'] = 0\n",
    "    train_df.loc[train_df.interest_level == 'high', 'is_high'] = 1\n",
    "    \n",
    "    train_df.loc[:, 'is_medium'] = 0\n",
    "    train_df.loc[train_df.interest_level == 'medium', 'is_medium'] = 1\n",
    "    \n",
    "    train_df.loc[:, 'is_low'] = 0\n",
    "    train_df.loc[train_df.interest_level == 'low', 'is_low'] = 1\n",
    "    \n",
    "    \n",
    "    print 'Extracting TFIDF features from \"description\"...'\n",
    "    TF_IDF_vectorizer_desc = TfidfVectorizer(stop_words='english', max_features=500, tokenizer=LemmaTokenizer())\n",
    "    desc = TF_IDF_vectorizer_desc.fit_transform(train_df.description)\n",
    "    \n",
    "    lda_desc = LatentDirichletAllocation(n_topics=n_desc_topics, max_iter=20)\n",
    "    desc_coded = lda_desc.fit_transform(desc)\n",
    "    \n",
    "    for col_idx in range(n_desc_topics):\n",
    "        train_df.loc[:, 'desc'+ str(col_idx)] = desc_coded[:, col_idx]\n",
    "        all_features.append('desc'+ str(col_idx))\n",
    "    \n",
    "    models_dict['TFIDF_desc'] = TF_IDF_vectorizer_desc\n",
    "    models_dict['LDA_desc'] = lda_desc\n",
    "\n",
    "    print 'Creating probability list for \"features\"...'\n",
    "    models_dict['features_probability_list'], models_dict['features_list'] = features_probability(train_df)\n",
    "    \n",
    "    print 'Encoding features for training data...'\n",
    "    train_df = add_feature_prob_columns(train_df, models_dict['features_probability_list'],\n",
    "                                        models_dict['features_list'])\n",
    "    \n",
    "    all_features.extend(['high_prob_features', 'medium_prob_features'] + \n",
    "                        ['feat'+str(i) for i in range(n_features)])\n",
    "\n",
    "    \n",
    "    for field in prob_trans_features:\n",
    "        print 'Transorming \"%s\"...'%field\n",
    "        models_dict[field+'_trans_df'] = transform_categerical_feature(train_df, field, high_prior_prob, \n",
    "                                                                       medium_prior_prob)\n",
    "\n",
    "        train_df = add_prob_training(train_df, high_prior_prob, medium_prior_prob, field)\n",
    "        all_features.extend(['high_prob_'+field, 'medium_prob_'+field])\n",
    "    \n",
    "    print 'Encoding categorical features...'\n",
    "    for field in label_encode_features:\n",
    "        models_dict[field + '_label_encoder'] = LabelEncoder()\n",
    "        models_dict[field + '_label_encoder'] = models_dict[field + '_label_encoder'].fit(list(train_df[field].values) +\n",
    "                                                                                       list(test_df[field].values))\n",
    "        train_df[field + '_encoded'] = models_dict[field + '_label_encoder'].transform(train_df[field].values)\n",
    "        all_features.append(field + '_encoded')\n",
    "    \n",
    "    return train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare varibales for testing\n",
    "def prepare_data(test_df):\n",
    "    global models_dict\n",
    "        \n",
    "    print 'Extracting numerical features...'\n",
    "    test_df.loc[:, 'price_per_bedroom'] = test_df.apply(lambda r: r['price'] / float(r['bedrooms']) \n",
    "                                               if r['bedrooms']>0 else r['price'], axis=1)\n",
    "\n",
    "    test_df.loc[:, 'bedroom_per_bathroom'] = test_df.apply(lambda r: r['bedrooms'] / float(r['bathrooms']) \n",
    "                                               if r['bathrooms']>0 else r['bedrooms'], axis=1)\n",
    "\n",
    "    test_df.loc[:, 'price_per_all_rooms'] = test_df.apply(lambda r: r['price'] / float(r['bathrooms']+r['bedrooms']) \n",
    "                                               if r['bedrooms']+r['bathrooms']>0 else r['price'], axis=1)\n",
    "\n",
    "    test_df.loc[:,'desc_length'] = test_df.description.apply(lambda x: len(x.split()))\n",
    "    test_df.loc[:,'features_length'] = test_df.features.apply(len)\n",
    "    test_df.loc[:,'photos_no'] = test_df.photos.apply(len)\n",
    "\n",
    "    test_df.loc[:,'created'] = pd.to_datetime(test_df.loc[:, 'created'])\n",
    "    test_df.loc[:,'day'] = test_df.created.apply(lambda d: d.day)\n",
    "    test_df.loc[:,'hour'] = test_df.created.apply(lambda d: d.hour)\n",
    "    \n",
    "    print 'Encoding and extracting features from \"display address\"...'\n",
    "    test_df['disp_addr'] = test_df.display_address.apply(lambda r : code_addr(r, models_dict['addr_coding_list']))\n",
    "    \n",
    "    print 'Adding location features...'\n",
    "    test_df = add_location_features(test_df)\n",
    "    \n",
    "    print 'Pairing features...'\n",
    "    test_df['t_price'] = np.ceil(test_df.price / 100)\n",
    "    \n",
    "    test_df['manager_id_area'] = test_df.manager_id + '-' + test_df.area.astype('str') \n",
    "    test_df['manager_id_building_id'] = test_df.manager_id + '-' + test_df.building_id\n",
    "    test_df['manager_id_display_address'] = test_df.manager_id + '-' + test_df.display_address\n",
    "    test_df['manager_id_price'] = test_df.manager_id + '-' + test_df.t_price.astype('str')\n",
    "    \n",
    "    test_df['disp_addr_price'] = test_df.disp_addr.astype('str') + '-' + test_df.t_price.astype('str')\n",
    "    test_df['display_address_price'] = test_df.display_address + '-' + test_df.t_price.astype('str')\n",
    "    test_df['area_price'] = test_df.area.astype('str') + '-' + test_df.t_price.astype('str')\n",
    "    test_df['building_id_price'] = test_df.building_id + '-' + test_df.t_price.astype('str')\n",
    "    \n",
    "    test_df['manager_id_disp_addr_price'] = test_df.manager_id + '-' + test_df.disp_addr.astype('str') + '-' + test_df.t_price.astype('str')\n",
    "    test_df['manager_id_building_id_price'] = test_df.manager_id + '-' + test_df.building_id + '-' + test_df.t_price.astype('str')\n",
    "    test_df['manager_id_area_price'] = test_df.manager_id + '-' + test_df.area.astype('str') + '-' + test_df.t_price.astype('str')\n",
    "    \n",
    "    print 'Encoding features probability...'\n",
    "    test_df = add_feature_prob_columns(test_df,  models_dict['features_probability_list'], \n",
    "                                       models_dict['features_list'])\n",
    "    \n",
    "    print 'Extracting TFIDF features from \"description\"...'\n",
    "    desc = models_dict['TFIDF_desc'].transform(test_df.description)\n",
    "    \n",
    "    desc_coded = models_dict['LDA_desc'].transform(desc)\n",
    "    \n",
    "    for col_idx in range(n_desc_topics):\n",
    "        test_df.loc[:, 'desc'+ str(col_idx)] = desc_coded[:, col_idx]\n",
    "    \n",
    "    for field in prob_trans_features:\n",
    "        print 'Transorming \"%s\"...'%field\n",
    "        test_df = add_prob_columns(test_df, models_dict[field + '_trans_df'], field)\n",
    "    \n",
    "    print 'Encoding categorical features...'\n",
    "    for field in label_encode_features:\n",
    "        test_df[field + '_encoded'] = models_dict[field + '_label_encoder'].transform(test_df[field].values)\n",
    "    \n",
    "\n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Oversampling and Undersampling to balance the data\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "\n",
    "def oversample_bin(df, features, target, ratio):   \n",
    "    smote = ADASYN(ratio=ratio, random_state=np.random.randint(1000))\n",
    "    s, y = smote.fit_sample(df[features+['weight']].values, df[target].values)\n",
    "    new_df = pd.DataFrame(s, columns=features+['weight'])\n",
    "    new_df[target] = y\n",
    "\n",
    "    int_features = ['bathrooms', 'bedrooms', 'desc_length', 'features_length', \n",
    "                    'photos_no', 'day', 'hour']+ ['feat'+str(i) for i in range(n_features)] \n",
    "\n",
    "    for f in int_features:\n",
    "        new_df.loc[:, f] = np.round( new_df.loc[:, f].values)\n",
    "\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def undersample_random(df, features, target, ratio):\n",
    "    undersample = RandomUnderSampler(ratio=ratio, random_state=np.random.randint(1000))\n",
    "    s, y = undersample.fit_sample(df[features+['weight']].values, df[target].values)\n",
    "    new_df = pd.DataFrame(s, columns=features+['weight'])\n",
    "    new_df[target] = y\n",
    "    return new_df\n",
    "\n",
    "def undersample_tomek(df, features, target):    \n",
    "    TL_hl = TomekLinks(random_state=np.random.randint(1000))\n",
    "    s, y = TL_hl.fit_sample(df[features+['weight']], df[target].values)\n",
    "    new_df = pd.DataFrame(s, columns=features+['weight'])\n",
    "    new_df[target] = y\n",
    "    return new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_prediction(in_data, out_data, model, prefix):\n",
    "    pred_proba = model.predict_proba(in_data)\n",
    "    \n",
    "    idx = {col:idx for (idx, col) in  enumerate(model.classes_)}\n",
    "\n",
    "    for col in model.classes_:\n",
    "        out_data.loc[:, prefix + str(col) ] = pred_proba[:, idx[col]]\n",
    "        \n",
    "    return  out_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: (49352, 15)\n",
      "Test size: (74659, 14)\n"
     ]
    }
   ],
   "source": [
    "f = ZipFile('./train.json.zip')\n",
    "data = pd.read_json(f.open(f.filelist[0]))\n",
    "\n",
    "f = ZipFile('./test.json.zip')\n",
    "test = pd.read_json(f.open(f.filelist[0]))\n",
    "\n",
    "print 'Training size:', data.shape\n",
    "print 'Test size:', test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting numerical features...\n",
      "Building coding list for display address...\n",
      "Encoding and extracting features from \"display address\"...\n",
      "Adding location features...\n",
      "Pairing features...\n",
      "Extracting TFIDF features from \"description\"...\n",
      "Creating probability list for \"features\"...\n",
      "Encoding features for training data...\n",
      "Transorming \"disp_addr\"...\n",
      "Transorming \"building_id\"...\n",
      "Transorming \"manager_id\"...\n",
      "Transorming \"area\"...\n",
      "Transorming \"manager_id_area\"...\n",
      "Transorming \"area_price\"...\n",
      "Encoding categorical features...\n"
     ]
    }
   ],
   "source": [
    "data = prepare_data_first(data, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['disp_addr_trans_df',\n",
       " 'features_probability_list',\n",
       " 'street_address_label_encoder',\n",
       " 'manager_id_trans_df',\n",
       " 'building_id_label_encoder',\n",
       " 'area_price_trans_df',\n",
       " 'features_list',\n",
       " 'LDA_desc',\n",
       " 'TFIDF_desc',\n",
       " 'manager_id_area_trans_df',\n",
       " 'addr_coding_list',\n",
       " 'manager_id_label_encoder',\n",
       " 'building_id_trans_df',\n",
       " 'area_trans_df']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stacked model. Converting the problem to binary and coding it in different ways.\n",
    "\n",
    "class XGB_Bin_OvR:\n",
    "    def __init__(self):\n",
    "        self.n_codes = 9\n",
    "        self.models = {}\n",
    "        self.fields_and_models = zip(['m'+str(i) for i in range(self.n_codes)], \n",
    "                                     ['model'+str(i) for i in range(self.n_codes)],\n",
    "                                     [0, 0, 0, 1, 1, 1, 2, 2, 2])\n",
    "\n",
    "    def fit_one_fold(self, data):\n",
    "        h_ecoc = {f:v for f,v in zip(['m'+str(i) for i in  range(self.n_codes)], [1, 0, 0, 1, 0, 0, 1, 0, 0])}\n",
    "        m_ecoc = {f:v for f,v in zip(['m'+str(i) for i in  range(self.n_codes)], [0, 1, 0, 0, 1, 0, 0, 1, 0])}\n",
    "        l_ecoc = {f:v for f,v in zip(['m'+str(i) for i in  range(self.n_codes)], [0, 0, 1, 0, 0, 1, 0, 0, 1])}\n",
    "        \n",
    "        data[['m'+str(i) for i in  range(self.n_codes)]] = data.interest_level.apply(lambda x: pd.Series(h_ecoc) \n",
    "                                                                                if x=='high' \n",
    "                                                                                else \n",
    "                                                                                (pd.Series(m_ecoc \n",
    "                                                                                           if x=='medium' \n",
    "                                                                                           else \n",
    "                                                                                           pd.Series(l_ecoc))))\n",
    "        for cls, model_name, trans_flag in self.fields_and_models:\n",
    "            print 'Training: %s using %d...'%(model_name, trans_flag)\n",
    "\n",
    "            n1 = len(data.loc[data[cls] == 1])\n",
    "            n2 = len(data) - n1\n",
    "            r = min([n1, n2]) /float(max([n1, n2]))\n",
    "\n",
    "            if trans_flag == 0:\n",
    "                in_data = undersample_tomek(data, all_features, cls)\n",
    "\n",
    "            elif trans_flag == 1:\n",
    "                r *= np.random.uniform(1.5, 3) if r < 0.334 else 1.0 \n",
    "                r = 0.5 if r < 0.5 else r\n",
    "                in_data = undersample_random(data, all_features, cls, r)\n",
    "\n",
    "            elif trans_flag == 2:\n",
    "                r *= (1.5 + np.random.uniform(0, 0.3)) \n",
    "                r = 0.5 if r > 0.5 else r\n",
    "                in_data = oversample_bin(data, all_features, cls, r)\n",
    "\n",
    "\n",
    "            self.models[model_name] = XGBClassifier(reg_alpha=0.5, reg_lambda=0.25, #scale_pos_weight=w,\n",
    "                                               n_estimators=1000, max_depth=4, subsample=0.75, max_delta_step=1,\n",
    "                                               learning_rate=0.05, colsample_bytree=0.75, objective='binary:logistic')\n",
    "\n",
    "            self.models[model_name] = self.models[model_name].fit(in_data[all_features].values, in_data[cls].values, \n",
    "                                                        eval_metric='logloss', sample_weight=in_data.weight.values)\n",
    "    def predict(self, data):\n",
    "        proba_df = pd.DataFrame()\n",
    "        for col_name, model_name, _ in self.fields_and_models:\n",
    "            model_input4 = data[all_features].values\n",
    "            proba_df = make_prediction(model_input4, proba_df, self.models[model_name], col_name + '_')\n",
    "        return proba_df\n",
    "    \n",
    "    def fit_predict(self, data, kfold):\n",
    "        tmp = pd.DataFrame()\n",
    "        i = 0\n",
    "        for itrain, itest in kfold.values():\n",
    "            print 'Training fold %d ...' % i\n",
    "            i += 1\n",
    "            self.fit_one_fold(data.iloc[itrain])\n",
    "            x = self.predict(data.iloc[itest])\n",
    "            x['interest_level'] = data.iloc[itest]['interest_level']\n",
    "            tmp = tmp.append(x)\n",
    "            print '--------------------------------------------------'\n",
    "            \n",
    "        self.fit_one_fold(data)\n",
    "        return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class XGB_Bin_OvO:\n",
    "    def __init__(self):\n",
    "        self.models = {}\n",
    "        self.exclude_list = zip(['low', 'high', 'medium'], ['is_high', 'is_medium', 'is_low'])\n",
    "\n",
    "    def fit_one_fold(self, data):\n",
    "        for exclude, cls in self.exclude_list:\n",
    "            model_name = 'not_' + exclude\n",
    "            print 'Training: %s...'% model_name\n",
    "            \n",
    "            in_data = data.loc[data.interest_level != exclude]\n",
    "            \n",
    "            n1 = len(in_data.loc[in_data[cls] == 1])\n",
    "            n2 = len(in_data) - n1\n",
    "            r = min([n1, n2]) /float(max([n1, n2]))\n",
    "            \n",
    "            in_data = SMOTE_tomek(in_data, all_features, cls, r)\n",
    "\n",
    "            self.models[model_name] = XGBClassifier(reg_alpha=0.5, reg_lambda=0.25, #scale_pos_weight=w,\n",
    "                                               n_estimators=1000, max_depth=4, subsample=0.75, max_delta_step=1,\n",
    "                                               learning_rate=0.05, colsample_bytree=0.75, objective='binary:logistic')\n",
    "\n",
    "            self.models[model_name] = self.models[model_name].fit(in_data[all_features].values, in_data[cls].values, \n",
    "                                                        eval_metric='logloss')\n",
    "    def predict(self, data):\n",
    "        proba_df = pd.DataFrame()\n",
    "        for exclude, cls in self.exclude_list:\n",
    "            model_input4 = data[all_features].values\n",
    "            proba_df = make_prediction(model_input4, proba_df, self.models['not_'+exclude], 'not_' + exclude + '_')\n",
    "        return proba_df\n",
    "    \n",
    "    def fit_predict(self, data, kfold):\n",
    "        tmp = pd.DataFrame()\n",
    "        i = 0\n",
    "        for itrain, itest in kfold.values():\n",
    "            print 'Training fold %d ...' % i\n",
    "            i += 1\n",
    "            self.fit_one_fold(data.iloc[itrain])\n",
    "            x = self.predict(data.iloc[itest])\n",
    "            x['interest_level'] = data.iloc[itest]['interest_level']\n",
    "            tmp = tmp.append(x)\n",
    "            print '--------------------------------------------------'\n",
    "            \n",
    "        self.fit_one_fold(data)\n",
    "        return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class XGB_Multi:\n",
    "    def __init__(self):\n",
    "        self.models = None\n",
    "        self.comb_features =None\n",
    "        \n",
    "    def fit_one_fold(self, data):\n",
    "        self.comb_features = list(data.columns)\n",
    "        self.comb_features.remove('interest_level')\n",
    "        \n",
    "        self.model = XGBClassifier(reg_alpha=0.5, reg_lambda=0.25, #scale_pos_weight=w,\n",
    "                                   n_estimators=2000, max_depth=4, subsample=0.75, max_delta_step=1,\n",
    "                                   learning_rate=0.03, colsample_bytree=0.75, objective='multi:softprob')\n",
    "        self.model = self.model.fit(data[all_features].values, data.interest_level.values, eval_metric='mlogloss',\n",
    "                                    sample_weight=data.weight.values)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        proba_df = pd.DataFrame()\n",
    "        model_input4 = data[all_features].values\n",
    "        proba_df = make_prediction(model_input4, proba_df, self.model, '')\n",
    "        return proba_df\n",
    "    \n",
    "    def fit_predict(self, data, kfold):\n",
    "        tmp = pd.DataFrame()\n",
    "        i = 0\n",
    "        for itrain, itest in kfold.values():\n",
    "            print 'Training fold %d ...' % i\n",
    "            i += 1\n",
    "            self.fit_one_fold(data.iloc[itrain])\n",
    "            x = self.predict(data.iloc[itest])\n",
    "            x['interest_level'] = data.iloc[itest]['interest_level']\n",
    "            tmp = tmp.append(x)\n",
    "            \n",
    "        self.fit_one_fold(data)\n",
    "        return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Comb_RF:\n",
    "    def __init__(self):\n",
    "        self.models = None\n",
    "        self.comb_features = None\n",
    "        \n",
    "    def fit_one_fold(self, data):\n",
    "        self.comb_features = list(data.columns)\n",
    "        self.comb_features.remove('interest_level')\n",
    "        \n",
    "        self.model = RandomForestClassifier(max_depth=6, n_estimators=300, max_features='auto')\n",
    "        self.model = self.model.fit(data[self.comb_features].values, data.interest_level.values)\n",
    "        \n",
    "    def predict(self, data):\n",
    "        proba_df = pd.DataFrame()\n",
    "        model_input4 = data[self.comb_features].values\n",
    "        proba_df = make_prediction(model_input4, proba_df, self.model, '')\n",
    "        return proba_df\n",
    "    \n",
    "    def fit_predict(self, data, kfold):\n",
    "        tmp = pd.DataFrame()\n",
    "        i = 0\n",
    "        for itrain, itest in kfold.values():\n",
    "            print 'Training fold %d ...' % i\n",
    "            i += 1\n",
    "            self.fit_one_fold(data.iloc[itrain])\n",
    "            x = self.predict(data.iloc[itest])\n",
    "            x['interest_level'] = data.iloc[itest]['interest_level']\n",
    "            tmp = tmp.append(x)\n",
    "            \n",
    "        self.fit_one_fold(data)\n",
    "        return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "i = 0\n",
    "kfold = {}\n",
    "for itrain, itest in skf.split(np.zeros(len(data)), data['interest_level']):\n",
    "    kfold[i] = (itrain, itest)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['weight'] = data.interest_level.apply(lambda x: np.random.uniform(-0.2,0.2) + \n",
    "                                    2.5 if x=='high' else(1.5 if x=='medium' else 1.1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0 ...\n",
      "Training: model0 using 0...\n",
      "Training: model1 using 0...\n",
      "Training: model2 using 0...\n",
      "Training: model3 using 1...\n",
      "Training: model4 using 1...\n",
      "Training: model5 using 1...\n",
      "Training: model6 using 2...\n",
      "Training: model7 using 2...\n",
      "Training: model8 using 2...\n",
      "--------------------------------------------------\n",
      "Training fold 1 ...\n",
      "Training: model0 using 0...\n",
      "Training: model1 using 0...\n",
      "Training: model2 using 0...\n",
      "Training: model3 using 1...\n",
      "Training: model4 using 1...\n",
      "Training: model5 using 1...\n",
      "Training: model6 using 2...\n",
      "Training: model7 using 2...\n",
      "Training: model8 using 2...\n",
      "--------------------------------------------------\n",
      "Training fold 2 ...\n",
      "Training: model0 using 0...\n",
      "Training: model1 using 0...\n",
      "Training: model2 using 0...\n",
      "Training: model3 using 1...\n",
      "Training: model4 using 1...\n",
      "Training: model5 using 1...\n",
      "Training: model6 using 2...\n",
      "Training: model7 using 2...\n",
      "Training: model8 using 2...\n",
      "--------------------------------------------------\n",
      "Training fold 3 ...\n",
      "Training: model0 using 0...\n",
      "Training: model1 using 0...\n",
      "Training: model2 using 0...\n",
      "Training: model3 using 1...\n",
      "Training: model4 using 1...\n",
      "Training: model5 using 1...\n",
      "Training: model6 using 2...\n",
      "Training: model7 using 2...\n",
      "Training: model8 using 2...\n",
      "--------------------------------------------------\n",
      "Training fold 4 ...\n",
      "Training: model0 using 0...\n",
      "Training: model1 using 0...\n",
      "Training: model2 using 0...\n",
      "Training: model3 using 1...\n",
      "Training: model4 using 1...\n",
      "Training: model5 using 1...\n",
      "Training: model6 using 2...\n",
      "Training: model7 using 2...\n",
      "Training: model8 using 2...\n",
      "--------------------------------------------------\n",
      "Training: model0 using 0...\n",
      "Training: model1 using 0...\n",
      "Training: model2 using 0...\n",
      "Training: model3 using 1...\n",
      "Training: model4 using 1...\n",
      "Training: model5 using 1...\n",
      "Training: model6 using 2...\n",
      "Training: model7 using 2...\n",
      "Training: model8 using 2...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "xgb_bin_ovr = XGB_Bin_OvR()   \n",
    "xgb_bin_ovr_out = xgb_bin_ovr.fit_predict(data, kfold)\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0 ...\n",
      "Training fold 1 ...\n",
      "Training fold 2 ...\n",
      "Training fold 3 ...\n",
      "Training fold 4 ...\n"
     ]
    }
   ],
   "source": [
    "comb_ovr = Comb_RF()\n",
    "ovr_out = comb_ovr.fit_predict(xgb_bin_ovr_out, kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0 ...\n",
      "Training: not_low...\n",
      "Training: not_high...\n",
      "Training: not_medium...\n",
      "--------------------------------------------------\n",
      "Training fold 1 ...\n",
      "Training: not_low...\n",
      "Training: not_high...\n",
      "Training: not_medium...\n",
      "--------------------------------------------------\n",
      "Training fold 2 ...\n",
      "Training: not_low...\n",
      "Training: not_high...\n",
      "Training: not_medium...\n",
      "--------------------------------------------------\n",
      "Training fold 3 ...\n",
      "Training: not_low...\n",
      "Training: not_high...\n",
      "Training: not_medium...\n",
      "--------------------------------------------------\n",
      "Training fold 4 ...\n",
      "Training: not_low...\n",
      "Training: not_high...\n",
      "Training: not_medium...\n",
      "--------------------------------------------------\n",
      "Training: not_low...\n",
      "Training: not_high...\n",
      "Training: not_medium...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "xgb_bin_ovo = XGB_Bin_OvO()   \n",
    "xgb_bin_ovo_out = xgb_bin_ovo.fit_predict(data, kfold)\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0 ...\n",
      "Training fold 1 ...\n",
      "Training fold 2 ...\n",
      "Training fold 3 ...\n",
      "Training fold 4 ...\n"
     ]
    }
   ],
   "source": [
    "comb_ovo = Comb_RF()\n",
    "ovo_out = comb_ovo.fit_predict(xgb_bin_ovo_out, kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['weight'] = data.interest_level.apply(lambda x: np.random.uniform(-0.2,0.2) + \n",
    "                                    1.5 if x=='high' else(1.2 if x=='medium' else 1.1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0 ...\n",
      "Training fold 1 ...\n",
      "Training fold 2 ...\n",
      "Training fold 3 ...\n",
      "Training fold 4 ...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "xgb_multi = XGB_Multi()   \n",
    "xgb_multi_out = xgb_multi.fit_predict(data, kfold)\n",
    "print 'Done.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def change_col_name(l, suffix):\n",
    "    return [x + suffix if x in ['high', 'low', 'medium'] else x for x in l]\n",
    "\n",
    "del xgb_multi_out['interest_level']\n",
    "del ovo_out['interest_level']\n",
    "comb_in = pd.concat([ovr_out, ovo_out, xgb_multi_out], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comb_model= Comb_RF()\n",
    "comb_model.fit_one_fold(comb_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting numerical features...\n",
      "Encoding and extracting features from \"display address\"...\n",
      "Adding location features...\n",
      "Pairing features...\n",
      "Encoding features probability...\n",
      "Extracting TFIDF features from \"description\"...\n",
      "Transorming \"disp_addr\"...\n",
      "Transorming \"building_id\"...\n",
      "Transorming \"manager_id\"...\n",
      "Transorming \"area\"...\n",
      "Transorming \"manager_id_area\"...\n",
      "Transorming \"area_price\"...\n",
      "Encoding categorical features...\n",
      "(74659, 114)\n"
     ]
    }
   ],
   "source": [
    "d2  = prepare_data(test)\n",
    "print d2[all_features].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ovr_prob_df = xgb_bin_ovr.predict(d2)\n",
    "ovr_prob_df = comb_ovr.predict(ovr_prob_df)\n",
    "\n",
    "ovo_prob_df = xgb_bin_ovo.predict(d2)\n",
    "ovo_prob_df = comb_ovo.predict(ovo_prob_df)\n",
    "\n",
    "multi_prob_df = xgb_multi.predict(d2)\n",
    "\n",
    "comb_in = pd.concat([ovr_prob_df, ovo_prob_df, multi_prob_df], axis=1)\n",
    "\n",
    "prob_df = comb_model.predict(comb_in)\n",
    "\n",
    "prob_df[\"listing_id\"] = d2.listing_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob_df[['listing_id', 'high', 'low', 'medium']].to_csv('Submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
